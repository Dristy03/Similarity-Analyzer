{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer \n",
    "import numpy\n",
    "import json\n",
    "import os\n",
    "import tflearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.path.dirname(os.path.realpath('__file__'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH + \"\\\\file.txt\") as file:\n",
    "    dataset = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_words = []\n",
    "labels = []\n",
    "docs_x = [] #List of all the question_patterns.\n",
    "docs_y = [] #List of all the tags for specific Texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for intent in dataset[\"intents\"]:\n",
    "    for pattern in intent[\"texts\"]: #Stems the words. Finds the root of the word. Removes extra characters and symbols to find the root word. \n",
    "        split_words = nltk.word_tokenize(pattern) #Tokenizes the words. Breakes the words in the places of spaces and returns a list of all the words in it.\n",
    "        list_words.extend(split_words) #Using instead of looping and adding one word at a time in the list. It just extends the list untill all the words are in it.\n",
    "        docs_x.append(split_words) #Adding the pattern of words inside docs_x list.\n",
    "        docs_y.append(intent[\"tag\"]) #For each pattern, it says what Tag it is a part of.\n",
    "\n",
    "    if intent[\"tag\"] not in labels:\n",
    "        labels.append(intent[\"tag\"]) #Adds all the tags in the labels list.\n",
    "\n",
    "list_words = [stemmer.stem(w.lower()) for w in list_words if w != \"?\"] #Lower cases all the words in the list_words list. \n",
    "list_words = sorted(list(set(list_words))) #Makes a set of the words to remove duplicate. This gives us the actual vocabulary size of the intent. Then converts it back to list and sorts it. \n",
    "\n",
    "labels = sorted(labels) #Sorts the labels where the tags are stored.\n",
    "\n",
    "training = [] #contains the bag of words.\n",
    "output = [] #The output list to choose the right tag for the output.\n",
    "\n",
    "out_empty = [0 for _ in range(len(labels))]\n",
    "\n",
    "for x, doc in enumerate(docs_x):\n",
    "    bag = [] #Makes a list of all the words and tells if those words are occuring to find a pattern.\n",
    "    #We are representing each sentence with a list the length of the amount of words in our models vocabulary. \n",
    "    #Each position in the list will represent a word from our vocabulary. \n",
    "    #If the position in the list is a 1 then that will mean that the word exists in our sentence, if it is a 0 then the word is nor present. \n",
    "\n",
    "    split_words = [stemmer.stem(w.lower()) for w in doc] #Stems all the words inside docs_x list.\n",
    "\n",
    "    for w in list_words:\n",
    "        if w in split_words:\n",
    "            bag.append(1) #we are putting 1 in the bag of words list for the word (already present in the vocabulary list_words) present in the pattern and 0 if it is not.\n",
    "        else:\n",
    "            bag.append(0)\n",
    "\n",
    "    output_row = out_empty[:]\n",
    "    output_row[labels.index(docs_y[x])] = 1 #We are looking in the labels list to find were the tag is in that list. We are making that value 1 in the output row. \n",
    "    #We will create output lists which are the length of the amount of labels/tags we have in our dataset. \n",
    "    #Each position in the list will represent one distinct label/tag, a 1 in any of those positions will show which label/tag is represented.\n",
    "\n",
    "    training.append(bag) #We are putting the bag of words in the training list. \n",
    "    output.append(output_row) #We are putting the output_row list in the output list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(s, list_words): #bag_of_words function will transform our string input to a bag of words using our created words list\n",
    "    bag = [0 for _ in range(len(list_words))]\n",
    "\n",
    "    inp_str_words = nltk.word_tokenize(s)\n",
    "    inp_str_words = [stemmer.stem(word.lower()) for word in inp_str_words]\n",
    "\n",
    "    for search_element in inp_str_words:\n",
    "        for i, w in enumerate(list_words):\n",
    "            if w == search_element:\n",
    "                bag[i] = 1 \n",
    "            \n",
    "    return numpy.array(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = numpy.array(training) #we will convert our training data and output to numpy arrays.\n",
    "output = numpy.array(output)\n",
    "#tensorflow.reset_default_graph() #Resetting all the previous stuffs in the graph.\n",
    "\n",
    "net = tflearn.input_data(shape=[None, len(training[0])]) #This finds the input shape that we are expecting for the model. Each training input is gonna be of the same length, so, 0.\n",
    "net = tflearn.fully_connected(net, 8) #Hidden layer with 8 neurons.\n",
    "net = tflearn.fully_connected(net, 8) #Another hidden layer with 8 neurons.\n",
    "net = tflearn.fully_connected(net, 8) #Another hidden layer with 8 neurons.\n",
    "net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\") #length will be how many labels (how many tags) we have. Give us probability for each neuron in the network. The higest probability will be the output.\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "model = tflearn.DNN(net) #Model gets trained.\n",
    "\n",
    "try:\n",
    "    model.predict(PATH + \"\\\\model.tflearn\")\n",
    "    model.load()\n",
    "except:\n",
    "    model.fit(training, output, n_epoch=2000, batch_size=8, show_metric=True) #Fitting our data to the model. The number of epochs we set is the amount of times that the model will see the same information while training.\n",
    "    model.save(PATH + \"\\\\model.tflearn\") #we can save it to the file model.tflearn for use in other scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check():\n",
    "     \n",
    "    print(\"Type an answer (Enter 'quit' to stop)\")\n",
    "    while True:\n",
    "        inp = input(\"You: \")\n",
    "        inp=inp.lower()\n",
    "        if inp == \"quit\":\n",
    "            break\n",
    "\n",
    "        results = model.predict([bag_of_words(inp, list_words)]) #we predict the most matching text from the training dataset that matches with the input text.\n",
    "        results_index = numpy.argmax(results) #we get thr index of the result.\n",
    "        tag = labels[results_index] #we get the tag of the most matching text with the input text.\n",
    "        \n",
    "        for tg in dataset[\"intents\"]:\n",
    "            if tg['tag'] == tag:\n",
    "                response = tg['texts']\n",
    "        print(\"Most similar text to the input: \")\n",
    "        print(response) #we show the most matching text with the input text. \n",
    "        split_sent_resp = nltk.sent_tokenize(response) #we divide the response text (the most matching text with the input text)\n",
    "        list_sents=split_sent_resp #we put the sentenses in a different list for future work.\n",
    "\n",
    "        split_sent_inp = nltk.sent_tokenize(inp) #we split the input text into sentenses\n",
    "        len_inp=len(split_sent_inp) #number of sentenses in the input text.\n",
    "        list_sents.extend(split_sent_inp)\n",
    "        total_len=len(list_sents) #we get the total number of sentenses in input and response text, including the duplicate sentenses.\n",
    "        set_sents=set(list_sents) #we are making the list a set, so that duplicate sentenses get deleted.\n",
    "        set_len=len(set_sents)\n",
    "        len_difference=total_len-set_len #this gives us the number of duplicate sentenses, i.e, the copied or plaigarized sentenses.\n",
    "        plag_percent=(1-((len_inp-len_difference)/len_inp))*100\n",
    "        print(\"The percent of plagarism in the document is: \",plag_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c6c6997f7e37e6f8c76f5b26962cb660ccbb78f7e53c030cd3d7dae3ca94b3a1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
